#!/usr/bin/env python3
"""
ë…ë¦½ì ì¸ ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
JSONL í˜•íƒœì˜ Alpaca ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ì—¬ identifiers ë¶„í¬ì™€ í†µê³„ ì •ë³´ ì œê³µ
"""

import json
import argparse
import statistics
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional
import sys


def load_jsonl_dataset(file_path: Path) -> List[Dict[str, Any]]:
    """JSONL íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if not file_path.exists():
        print(f"âŒ File not found: {file_path}")
        return []

    dataset = []
    error_count = 0

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        entry = json.loads(line)
                        dataset.append(entry)
                    except json.JSONDecodeError as e:
                        error_count += 1
                        if error_count <= 5:  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                            print(f"âš ï¸ JSON decode error at line {line_num}: {e}")
                        elif error_count == 6:
                            print(f"âš ï¸ ... (ë” ë§ì€ JSON ì—ëŸ¬ê°€ ìˆìŠµë‹ˆë‹¤)")
    except Exception as e:
        print(f"âŒ Error reading file {file_path}: {e}")
        return []

    if error_count > 0:
        print(f"âš ï¸ Total JSON decode errors: {error_count}")

    print(f"âœ… Loaded {len(dataset)} valid entries from {file_path.name}")
    return dataset


def extract_output_data(entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """ì¶œë ¥ JSONì—ì„œ reasoningê³¼ identifiers ì¶”ì¶œ"""
    try:
        output_str = entry.get('output', '').strip()
        if not output_str:
            return None

        output_data = json.loads(output_str)
        if not isinstance(output_data, dict):
            return None

        if 'reasoning' not in output_data or 'identifiers' not in output_data:
            return None

        return {
            'reasoning': output_data['reasoning'],
            'identifiers': output_data['identifiers'],
            'raw_output': output_str
        }
    except (json.JSONDecodeError, TypeError, KeyError):
        return None


def categorize_sample(identifiers: List[str]) -> str:
    """ìƒ˜í”Œì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
    if not isinstance(identifiers, list):
        return "invalid"

    if len(identifiers) == 0:
        return "secure"  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ = ë³´ì•ˆì´ ì˜ ëœ ì½”ë“œ
    else:
        return "vulnerable"  # ë¹„ì–´ìˆì§€ ì•Šì€ ë¦¬ìŠ¤íŠ¸ = ì·¨ì•½í•œ ì½”ë“œ


def analyze_dataset_statistics(dataset: List[Dict[str, Any]], dataset_name: str = "Dataset") -> Dict[str, Any]:
    """ë°ì´í„°ì…‹ì˜ ìƒì„¸ í†µê³„ ë¶„ì„"""
    print(f"\nğŸ“Š {dataset_name} Analysis")
    print("=" * 70)

    stats = {
        'dataset_name': dataset_name,
        'total_entries': len(dataset),
        'valid_outputs': 0,
        'invalid_outputs': 0,
        'secure_samples': 0,  # ë¹ˆ identifiers
        'vulnerable_samples': 0,  # ë¹„ì–´ìˆì§€ ì•Šì€ identifiers
        'identifier_counts': Counter(),  # ê°œìˆ˜ë³„ ë¶„í¬
        'all_identifiers': [],
        'unique_identifiers': set(),
        'reasoning_lengths': [],
        'sample_categories': defaultdict(list),
        'identifier_frequency': Counter(),
    }

    # ê° ì—”íŠ¸ë¦¬ ë¶„ì„
    for i, entry in enumerate(dataset):
        output_data = extract_output_data(entry)

        if output_data is None:
            stats['invalid_outputs'] += 1
            continue

        stats['valid_outputs'] += 1

        identifiers = output_data['identifiers']
        reasoning = output_data['reasoning']

        # ìƒ˜í”Œ ë¶„ë¥˜
        category = categorize_sample(identifiers)
        stats['sample_categories'][category].append(i)

        # identifiers ë¶„ì„
        if isinstance(identifiers, list):
            if len(identifiers) == 0:
                stats['secure_samples'] += 1
            else:
                stats['vulnerable_samples'] += 1
                stats['identifier_counts'][len(identifiers)] += 1

                # ê°œë³„ identifier ë¶„ì„
                for identifier in identifiers:
                    if isinstance(identifier, str):
                        stats['all_identifiers'].append(identifier)
                        stats['unique_identifiers'].add(identifier)
                        stats['identifier_frequency'][identifier] += 1

        # reasoning ê¸¸ì´ ë¶„ì„
        if isinstance(reasoning, str):
            stats['reasoning_lengths'].append(len(reasoning))

    # ê²°ê³¼ ì¶œë ¥
    print_basic_statistics(stats)
    print_identifier_analysis(stats)
    print_vulnerability_analysis(stats)
    print_reasoning_analysis(stats)

    return stats


def print_basic_statistics(stats: Dict[str, Any]):
    """ê¸°ë³¸ í†µê³„ ì¶œë ¥"""
    print(f"ğŸ“‹ Basic Statistics:")
    print(f"  Total entries: {stats['total_entries']:,}")
    print(f"  Valid outputs: {stats['valid_outputs']:,}")
    print(f"  Invalid outputs: {stats['invalid_outputs']:,}")

    if stats['valid_outputs'] > 0:
        valid_pct = (stats['valid_outputs'] / stats['total_entries']) * 100
        print(f"  Success rate: {valid_pct:.1f}%")


def print_identifier_analysis(stats: Dict[str, Any]):
    """Identifier ë¶„ì„ ì¶œë ¥"""
    if stats['valid_outputs'] == 0:
        return

    secure_pct = (stats['secure_samples'] / stats['valid_outputs']) * 100
    vulnerable_pct = (stats['vulnerable_samples'] / stats['valid_outputs']) * 100

    print(f"\nğŸ” Sample Classification:")
    print(f"  Secure samples (empty identifiers): {stats['secure_samples']:,} ({secure_pct:.1f}%)")
    print(f"  Vulnerable samples (non-empty identifiers): {stats['vulnerable_samples']:,} ({vulnerable_pct:.1f}%)")

    if stats['vulnerable_samples'] > 0:
        print(f"\nğŸ“ˆ Identifier Count Distribution (Vulnerable Samples Only):")
        for count in sorted(stats['identifier_counts'].keys()):
            freq = stats['identifier_counts'][count]
            pct = (freq / stats['vulnerable_samples']) * 100
            print(f"  {count} identifiers: {freq:,} samples ({pct:.1f}%)")

        # í†µê³„ê°’ ê³„ì‚°
        all_counts = []
        for count, freq in stats['identifier_counts'].items():
            all_counts.extend([count] * freq)

        if all_counts:
            print(f"\nğŸ“Š Identifier Count Statistics (Vulnerable Samples):")
            print(f"  Min identifiers per sample: {min(all_counts)}")
            print(f"  Max identifiers per sample: {max(all_counts)}")
            print(f"  Mean identifiers per sample: {statistics.mean(all_counts):.1f}")
            print(f"  Median identifiers per sample: {statistics.median(all_counts):.1f}")


def print_vulnerability_analysis(stats: Dict[str, Any]):
    """ì·¨ì•½ì  ë¶„ì„ ì¶œë ¥"""
    if not stats['all_identifiers']:
        return

    print(f"\nğŸ† Most Frequent Vulnerability Identifiers:")
    for identifier, count in stats['identifier_frequency'].most_common(15):
        pct = (count / len(stats['all_identifiers'])) * 100
        print(f"  '{identifier}': {count:,} times ({pct:.1f}%)")

    print(f"\nğŸ“Š Identifier Diversity:")
    print(f"  Unique identifiers: {len(stats['unique_identifiers']):,}")
    print(f"  Total identifier instances: {len(stats['all_identifiers']):,}")

    if len(stats['unique_identifiers']) > 0:
        avg_frequency = len(stats['all_identifiers']) / len(stats['unique_identifiers'])
        print(f"  Average frequency per unique identifier: {avg_frequency:.1f}")


def print_reasoning_analysis(stats: Dict[str, Any]):
    """Reasoning ë¶„ì„ ì¶œë ¥"""
    if not stats['reasoning_lengths']:
        return

    print(f"\nğŸ“ Reasoning Text Analysis:")
    print(f"  Min length: {min(stats['reasoning_lengths']):,} characters")
    print(f"  Max length: {max(stats['reasoning_lengths']):,} characters")
    print(f"  Mean length: {statistics.mean(stats['reasoning_lengths']):.1f} characters")
    print(f"  Median length: {statistics.median(stats['reasoning_lengths']):.1f} characters")

    # ê¸¸ì´ ë¶„í¬
    length_ranges = [
        (0, 100, "Very Short"),
        (101, 200, "Short"),
        (201, 400, "Medium"),
        (401, 600, "Long"),
        (601, float('inf'), "Very Long")
    ]

    print(f"\nğŸ“ Reasoning Length Distribution:")
    for min_len, max_len, label in length_ranges:
        count = sum(1 for length in stats['reasoning_lengths']
                    if min_len <= length <= max_len)
        if count > 0:
            pct = (count / len(stats['reasoning_lengths'])) * 100
            range_str = f"{min_len}-{max_len}" if max_len != float('inf') else f"{min_len}+"
            print(f"  {label} ({range_str} chars): {count:,} samples ({pct:.1f}%)")


def compare_datasets(all_stats: List[Dict[str, Any]]):
    """ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë¹„êµ"""
    if len(all_stats) <= 1:
        return

    print(f"\nğŸ”„ Dataset Comparison")
    print("=" * 70)

    # í—¤ë” ì¶œë ¥
    print(f"{'Dataset':<20} {'Total':<8} {'Valid':<8} {'Secure':<8} {'Vulnerable':<10} {'Unique IDs':<10}")
    print("-" * 70)

    for stats in all_stats:
        name = stats['dataset_name'][:18]  # ì´ë¦„ ê¸¸ì´ ì œí•œ
        total = stats['total_entries']
        valid = stats['valid_outputs']
        secure = stats['secure_samples']
        vulnerable = stats['vulnerable_samples']
        unique_ids = len(stats['unique_identifiers'])

        print(f"{name:<20} {total:<8,} {valid:<8,} {secure:<8,} {vulnerable:<10,} {unique_ids:<10,}")

    # ë¹„ìœ¨ ë¹„êµ
    print(f"\nğŸ“Š Percentage Comparison:")
    print(f"{'Dataset':<20} {'Success%':<10} {'Secure%':<10} {'Vulnerable%':<12}")
    print("-" * 60)

    for stats in all_stats:
        name = stats['dataset_name'][:18]
        if stats['total_entries'] > 0:
            success_pct = (stats['valid_outputs'] / stats['total_entries']) * 100
        else:
            success_pct = 0

        if stats['valid_outputs'] > 0:
            secure_pct = (stats['secure_samples'] / stats['valid_outputs']) * 100
            vulnerable_pct = (stats['vulnerable_samples'] / stats['valid_outputs']) * 100
        else:
            secure_pct = vulnerable_pct = 0

        print(f"{name:<20} {success_pct:<10.1f} {secure_pct:<10.1f} {vulnerable_pct:<12.1f}")


def save_detailed_report(all_stats: List[Dict[str, Any]], output_file: Path):
    """ìƒì„¸ ë³´ê³ ì„œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # setì„ listë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
        serializable_stats = []
        for stats in all_stats:
            serializable = dict(stats)
            serializable['unique_identifiers'] = list(stats['unique_identifiers'])
            serializable['sample_categories'] = dict(stats['sample_categories'])
            serializable_stats.append(serializable)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_stats, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Detailed report saved to: {output_file}")
    except Exception as e:
        print(f"âŒ Failed to save report: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze Alpaca dataset statistics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python analyze_dataset.py                                    # Analyze output/combined_dataset.jsonl (default)
  python analyze_dataset.py dataset.jsonl                      # Analyze specific file
  python analyze_dataset.py dataset1.jsonl dataset2.jsonl --compare
  python analyze_dataset.py --all --compare                    # Analyze all datasets and compare
  python analyze_dataset.py --save-report stats_report.json   # Save report for default dataset
        """
    )

    parser.add_argument("files", nargs="*",
                        help="JSONL dataset files to analyze (default: output/combined_dataset.jsonl)")
    parser.add_argument("--all", action="store_true",
                        help="Analyze all available datasets (claude_only, gemini_only, combined)")
    parser.add_argument("--compare", action="store_true",
                        help="Compare multiple datasets (if more than one file provided)")
    parser.add_argument("--save-report", type=str, metavar="FILE",
                        help="Save detailed statistics report to JSON file")
    parser.add_argument("--quiet", action="store_true",
                        help="Only show summary without detailed analysis")

    args = parser.parse_args()

    print("ğŸ“Š Dataset Statistics Analyzer")
    print("=" * 50)

    # ë¶„ì„í•  íŒŒì¼ ëª©ë¡ ê²°ì •
    files_to_analyze = []

    if args.all:
        # ëª¨ë“  ë°ì´í„°ì…‹ ë¶„ì„
        potential_files = [
            "output/claude_only_dataset.jsonl",
            "output/gemini_only_dataset.jsonl",
            "output/combined_dataset.jsonl"
        ]
        for file_path_str in potential_files:
            file_path = Path(file_path_str)
            if file_path.exists():
                files_to_analyze.append(file_path_str)

        if not files_to_analyze:
            print("âŒ No dataset files found in output directory")
            sys.exit(1)

        # --all ì˜µì…˜ ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ ë¹„êµ ëª¨ë“œ í™œì„±í™”
        args.compare = True

    elif args.files:
        # ì‚¬ìš©ìê°€ ì§€ì •í•œ íŒŒì¼ë“¤
        files_to_analyze = args.files
    else:
        # ê¸°ë³¸ê°’: combined_dataset.jsonl
        default_file = "output/combined_dataset.jsonl"
        if Path(default_file).exists():
            files_to_analyze = [default_file]
            print(f"ğŸ“ Using default dataset: {default_file}")
        else:
            print(f"âŒ Default dataset not found: {default_file}")
            print("ğŸ’¡ Available options:")
            print("   --all                    # Analyze all available datasets")
            print("   <filename>               # Specify a dataset file")
            sys.exit(1)

    all_stats = []

    for file_path_str in files_to_analyze:
        file_path = Path(file_path_str)
        if not file_path.exists():
            print(f"âŒ File not found: {file_path}")
            continue

        dataset = load_jsonl_dataset(file_path)
        if not dataset:
            continue

        dataset_name = file_path.stem
        if not args.quiet:
            stats = analyze_dataset_statistics(dataset, dataset_name)
        else:
            # ê°„ë‹¨í•œ í†µê³„ë§Œ ê³„ì‚°
            stats = {'dataset_name': dataset_name, 'total_entries': len(dataset)}

        all_stats.append(stats)

    # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë¹„êµ
    if args.compare and len(all_stats) > 1:
        compare_datasets(all_stats)

    # ë³´ê³ ì„œ ì €ì¥
    if args.save_report:
        save_detailed_report(all_stats, Path(args.save_report))

    if not all_stats:
        print("âŒ No valid datasets found to analyze")
        sys.exit(1)

    print(f"\nâœ… Analysis completed for {len(all_stats)} dataset(s)")


if __name__ == "__main__":
    main()